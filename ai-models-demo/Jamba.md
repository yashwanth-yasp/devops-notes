---
date: 2025-09-09
Speaker: Minnu
tags:
  - ai
  - ust
  - llm
---
- When the context length increases, attention cause the training time of the transformer model will quadratically increase 
- To solve this state space model was invented. 
- The Mamba architecture depends on this model 
- Then they combined transformer architecture and Mamba architecture with a mixture of experts model which gave us the jamba model
	- mixture of experts allows only certain parts of the model to be activated 
- In transformer every token is compared to the previous token 
- but in this model only a certain part of expert token instead of hitting the previous token every time (doubtful about this)



